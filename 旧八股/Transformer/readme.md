Transformer内容包括:
1. PPT: 
https://docs.google.com/presentation/d/1mcBSHhNSngFS-RS22cbRxoIn2mLhvNPuEg0-6tKDajQ/edit?slide=id.p#slide=id.p
https://www.youtube.com/watch?v=zxQyTK8quyY
![encoder示意图](https://github.com/CiciLyn/llm_intern_0/blob/main/%E6%97%A7%E5%85%AB%E8%82%A1/Transformer/figures/encoder.png)
2. 论文复现代码. 
https://arxiv.org/pdf/1706.03762



杂七杂八：
Q1：为什么 self-attention 的计算复杂度是$O(N^2)$?
答: 按照最大的复杂度来看 -- 矩阵计算复杂度。其余都忽略不计。https://zhuanlan.zhihu.com/p/661804092

